### ESPAÑOL (ES)

La integración de Modelos de Lenguaje Grande (LLMs) en aplicaciones empresariales ha pasado de ser una novedad a una necesidad estratégica. LangChain se ha posicionado como el framework orquestador por excelencia, pero su uso en producción requiere patrones de diseño robustos que van mucho más allá de los tutoriales básicos de "Hello Custom Prompt". En este análisis técnico profundo, desglosaremos cómo construir, desplegar y escalar sistemas de IA generativa utilizando LangChain v0.1+ (LCEL) en un entorno TypeScript estricto, integrado con Express y Drizzle ORM.

#### 1. Arquitectura de Orquestación con LCEL

LangChain Expression Language (LCEL) es el nuevo estándar. Proporciona una sintaxis declarativa y "pipeable" para componer cadenas.
A diferencia de las clases `Chain` heredadas, LCEL ofrece streaming nativo y soporte asíncrono de primer nivel.

```typescript
// chain.ts
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const model = new ChatOpenAI({ modelName: "gpt-4-turbo" });
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "Eres un experto en SQL."],
  ["user", "{question}"],
]);

// Composición funcional
export const sqlChain = prompt.pipe(model).pipe(new StringOutputParser());
```

#### 2. Drizzle ORM + pgvector: RAG de Alto Rendimiento

El núcleo de cualquier sistema RAG (Retrieval-Augmented Generation) es la base de datos vectorial. PostgreSQL con `pgvector` es superior a soluciones propietarias porque mantiene tus datos y vectores en el mismo lugar (ACID Compliance).

Para escala real (millones de vectores), el índice HNSW (Hierarchical Navigable Small World) es obligatorio.

```typescript
// schema.ts
import { pgTable, text, vector, index } from "drizzle-orm/pg-core";
import { sql } from "drizzle-orm";

export const documents = pgTable(
  "documents",
  {
    id: text("id").primaryKey(),
    content: text("content").notNull(),
    embedding: vector("embedding", { dimensions: 1536 }).notNull(),
  },
  (table) => ({
    // Índice HNSW para búsqueda rápida aproximada
    embeddingIndex: index("embedding_idx").using(
      "hnsw",
      table.embedding.op("vector_cosine_ops"),
    ),
  }),
);
```

**Estrategia de Búsqueda Híbrida**:

1.  **Full-Text Search (tsvector)**: Para coincidencia exacta de palabras clave ("Contrato 2024").
2.  **Semantic Search (pgvector)**: Para capturar significado ("Documentos legales recientes").
3.  **Reciprocal Rank Fusion (RRF)**: Algoritmo para combinar ambos resultados.

#### 3. Agentes y Function Calling (Tool Binding)

Los modelos modernos no solo chatean; actúan. Usamos `bindTools` para exponer funciones tipadas con Zod al LLM.

```typescript
import { z } from "zod";
import { tool } from "@langchain/core/tools";

const weatherTool = tool(
  async ({ city }) => {
    return `El clima en ${city} es soleado.`;
  },
  {
    name: "get_weather",
    description: "Obtener clima actual",
    schema: z.object({ city: z.string() }),
  },
);

const agent = model.bindTools([weatherTool]);
```

El LLM decidirá si llamar a la herramienta. LangChain orquesta la ejecución, pasa el output de vuelta al modelo, y genera la respuesta final.

#### 4. Streaming HTTP en NestJS/Express

La latencia de GPT-4 es de 10-30 segundos. El usuario no puede esperar una pantalla en blanco.
Usamos Server-Sent Events (SSE) para enviar tokens a medida que se generan.

```typescript
// controller.ts
@Post('chat')
async chat(@Body() body: ChatDto, @Res() res: Response) {
  res.setHeader('Content-Type', 'text/event-stream');

  const stream = await chain.stream({ question: body.message });

  for await (const chunk of stream) {
    res.write(`data: ${JSON.stringify({ token: chunk })}\n\n`);
  }
  res.end();
}
```

#### 5. Observabilidad con LangSmith

Una IA en producción es una caja negra. ¿Por qué el bot respondió eso?
LangSmith permite "debuggear" la mente del LLM:

- Ver el prompt exacto final (después de inyectar variables).
- Medir latencia de cada paso (Retrieval vs Generation).
- Dataset de evaluación para detectar regresiones cuando cambias el prompt.

La construcción de sistemas de IA robustos es ingeniería de software tradicional aplicada a un nuevo paradigma probabilístico. Tipado estricto, pruebas y observabilidad son no-negociables.

---

### ENGLISH (EN)

Integrating Large Language Models (LLMs) into enterprise applications has shifted from a novelty to a strategic necessity. LangChain has positioned itself as the quintessential orchestrator framework, but its production usage requires robust design patterns that go far beyond basic "Hello Custom Prompt" tutorials. In this deep technical analysis, we will break down how to build, deploy, and scale generative AI systems using LangChain v0.1+ (LCEL) in a strict TypeScript environment, integrated with Express and Drizzle ORM.

#### 1. Orchestration Architecture with LCEL

LangChain Expression Language (LCEL) is the new standard. It provides a declarative and "pipeable" syntax for composing chains.
Unlike legacy `Chain` classes, LCEL offers native streaming and first-class async support.

```typescript
// chain.ts
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const model = new ChatOpenAI({ modelName: "gpt-4-turbo" });
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a SQL expert."],
  ["user", "{question}"],
]);

// Functional composition
export const sqlChain = prompt.pipe(model).pipe(new StringOutputParser());
```

#### 2. Drizzle ORM + pgvector: High-Performance RAG

The core of any RAG (Retrieval-Augmented Generation) system is the vector database. PostgreSQL with `pgvector` is superior to proprietary solutions because it keeps your data and vectors in the same place (ACID Compliance).

For real scale (millions of vectors), the HNSW (Hierarchical Navigable Small World) index is mandatory.

```typescript
// schema.ts
import { pgTable, text, vector, index } from "drizzle-orm/pg-core";
import { sql } from "drizzle-orm";

export const documents = pgTable(
  "documents",
  {
    id: text("id").primaryKey(),
    content: text("content").notNull(),
    embedding: vector("embedding", { dimensions: 1536 }).notNull(),
  },
  (table) => ({
    // HNSW index for fast approximate nearest neighbor search
    embeddingIndex: index("embedding_idx").using(
      "hnsw",
      table.embedding.op("vector_cosine_ops"),
    ),
  }),
);
```

**Hybrid Search Strategy**:

1.  **Full-Text Search (tsvector)**: For exact keyword matching ("Contract 2024").
2.  **Semantic Search (pgvector)**: To capture meaning ("Recent legal documents").
3.  **Reciprocal Rank Fusion (RRF)**: Algorithm to combine both results.

#### 3. Agents and Function Calling (Tool Binding)

Modern models don't just chat; they act. We use `bindTools` to expose Zod-typed functions to the LLM.

```typescript
import { z } from "zod";
import { tool } from "@langchain/core/tools";

const weatherTool = tool(
  async ({ city }) => {
    return `The weather in ${city} is sunny.`;
  },
  {
    name: "get_weather",
    description: "Get current weather",
    schema: z.object({ city: z.string() }),
  },
);

const agent = model.bindTools([weatherTool]);
```

The LLM decides whether to call the tool. LangChain orchestrates the execution, passes the output back to the model, and generates the final response.

#### 4. HTTP Streaming in NestJS/Express

GPT-4 latency is 10-30 seconds. The user cannot wait for a blank screen.
We use Server-Sent Events (SSE) to send tokens as they are generated.

```typescript
// controller.ts
@Post('chat')
async chat(@Body() body: ChatDto, @Res() res: Response) {
  res.setHeader('Content-Type', 'text/event-stream');

  const stream = await chain.stream({ question: body.message });

  for await (const chunk of stream) {
    res.write(`data: ${JSON.stringify({ token: chunk })}\n\n`);
  }
  res.end();
}
```

#### 5. Observability with LangSmith

AI in production is a black box. Why did the bot answer that?
LangSmith allows you to "debug" the LLM's mind:

- See the exact final prompt (after injecting variables).
- Measure latency of each step (Retrieval vs Generation).
- Evaluation datasets to detect regressions when you change the prompt.

Building robust AI systems is traditional software engineering applied to a new probabilistic paradigm. Strict typing, testing, and observability are non-negotiable.

---

### PORTUGUÊS (PT)

A integração de Grandes Modelos de Linguagem (LLMs) em aplicações empresariais passou de novidade a necessidade estratégica. O LangChain posicionou-se como o framework orquestrador por excelência, mas o seu uso em produção exige padrões de projeto robustos que vão muito além dos tutoriais básicos de "Hello Custom Prompt". Nesta análise técnica profunda, detalharemos como construir, implantar e escalar sistemas de IA generativa utilizando LangChain v0.1+ (LCEL) em um ambiente TypeScript estrito, integrado com Express e Drizzle ORM.

#### 1. Arquitetura de Orquestração com LCEL

LangChain Expression Language (LCEL) é o novo padrão. Fornece uma sintaxe declarativa e "pipeable" para compor cadeias.
Ao contrário das classes `Chain` herdadas, o LCEL oferece streaming nativo e suporte assíncrono de primeira classe.

```typescript
// chain.ts
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const model = new ChatOpenAI({ modelName: "gpt-4-turbo" });
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "Você é um especialista em SQL."],
  ["user", "{question}"],
]);

// Composição funcional
export const sqlChain = prompt.pipe(model).pipe(new StringOutputParser());
```

#### 2. Drizzle ORM + pgvector: RAG de Alto Desempenho

O núcleo de qualquer sistema RAG (Retrieval-Augmented Generation) é o banco de dados vetorial. O PostgreSQL com `pgvector` é superior às soluções proprietárias porque mantém seus dados e vetores no mesmo lugar (Conformidade ACID).

Para escala real (milhões de vetores), o índice HNSW (Hierarchical Navigable Small World) é obrigatório.

```typescript
// schema.ts
import { pgTable, text, vector, index } from "drizzle-orm/pg-core";
import { sql } from "drizzle-orm";

export const documents = pgTable(
  "documents",
  {
    id: text("id").primaryKey(),
    content: text("content").notNull(),
    embedding: vector("embedding", { dimensions: 1536 }).notNull(),
  },
  (table) => ({
    // Índice HNSW para busca rápida aproximada
    embeddingIndex: index("embedding_idx").using(
      "hnsw",
      table.embedding.op("vector_cosine_ops"),
    ),
  }),
);
```

**Estratégia de Busca Híbrida**:

1.  **Full-Text Search (tsvector)**: Para correspondência exata de palavras-chave ("Contrato 2024").
2.  **Semantic Search (pgvector)**: Para capturar significado ("Documentos legais recentes").
3.  **Reciprocal Rank Fusion (RRF)**: Algoritmo para combinar ambos os resultados.

#### 3. Agentes e Function Calling (Tool Binding)

Os modelos modernos não apenas conversam; eles agem. Usamos `bindTools` para expor funções tipadas com Zod ao LLM.

```typescript
import { z } from "zod";
import { tool } from "@langchain/core/tools";

const weatherTool = tool(
  async ({ city }) => {
    return `O clima em ${city} é ensolarado.`;
  },
  {
    name: "get_weather",
    description: "Obter clima atual",
    schema: z.object({ city: z.string() }),
  },
);

const agent = model.bindTools([weatherTool]);
```

O LLM decidirá se deve chamar a ferramenta. O LangChain orquestra a execução, passa a saída de volta ao modelo e gera a resposta final.

#### 4. Streaming HTTP em NestJS/Express

A latência do GPT-4 é de 10 a 30 segundos. O usuário não pode esperar por uma tela em branco.
Usamos Server-Sent Events (SSE) para enviar tokens à medida que são gerados.

```typescript
// controller.ts
@Post('chat')
async chat(@Body() body: ChatDto, @Res() res: Response) {
  res.setHeader('Content-Type', 'text/event-stream');

  const stream = await chain.stream({ question: body.message });

  for await (const chunk of stream) {
    res.write(`data: ${JSON.stringify({ token: chunk })}\n\n`);
  }
  res.end();
}
```

#### 5. Observabilidade com LangSmith

Uma IA em produção é uma caixa preta. Por que o bot respondeu aquilo?
O LangSmith permite "depurar" a mente do LLM:

- Ver o prompt final exato (após injetar variáveis).
- Medir a latência de cada etapa (Retrieval vs Generation).
- Dataset de avaliação para detectar regressões ao alterar o prompt.

A construção de sistemas de IA robustos é engenharia de software tradicional aplicada a um novo paradigma probabilístico. Tipagem estrita, testes e observabilidade são inegociáveis.
